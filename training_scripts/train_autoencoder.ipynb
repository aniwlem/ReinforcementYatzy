{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Do be able to import reinforcement_yatzy\n",
        "lib_path = os.path.abspath('..')\n",
        "if lib_path not in sys.path:\n",
        "    sys.path.append(lib_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "from reinforcement_yatzy.nn_models.autoencoders.scoreboard_autoencoder import  ScoreboardAutoencoder\n",
        "from reinforcement_yatzy.scoreboard_dataset.scoreboard_dataset import ScoreboardDataset\n",
        "from reinforcement_yatzy.yatzy.base_player import ABCYatzyPlayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_autoencoder(\n",
        "    autoencoder: ScoreboardAutoencoder,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    n_epochs: int,\n",
        "    loss_path: Path,\n",
        "    n_rising_until_break: int,\n",
        "    save_interval: int,\n",
        "    save_path: Path,\n",
        "):\n",
        "    '''Train the autoencoder with holdout validation'''\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs_val_rising = 0\n",
        "    train_losses = np.zeros([n_epochs])\n",
        "    val_losses = np.zeros([n_epochs])\n",
        "\n",
        "    for i_epoch in range(n_epochs):\n",
        "        # Training\n",
        "        with tqdm(train_loader, unit='batch') as tepoch:\n",
        "            sum_loss = 0\n",
        "            for i_batch, batch in enumerate(tepoch):\n",
        "                tepoch.set_description(f'Epoch {i_epoch}')\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = autoencoder(batch)\n",
        "                loss = criterion(outputs, batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                sum_loss += loss.item()\n",
        "\n",
        "                tepoch.set_postfix(loss=sum_loss / (i_batch + 1))\n",
        "\n",
        "        avg_train_loss = sum_loss / len(train_loader)\n",
        "        train_losses[i_epoch] = avg_train_loss\n",
        "\n",
        "        # Validation\n",
        "        with torch.no_grad():\n",
        "            sum_val_loss = 0\n",
        "            for batch in val_loader:\n",
        "                outputs = autoencoder(batch)\n",
        "                val_loss = criterion(outputs, batch)\n",
        "                sum_val_loss += val_loss\n",
        "\n",
        "        avg_val_loss = sum_val_loss / len(val_loader)\n",
        "        val_losses[i_epoch] = avg_val_loss\n",
        "\n",
        "        tqdm.write(\n",
        "            f'\\nEpoch {i_epoch} - Training Loss: {avg_train_loss:.2e} - Validation Loss: {avg_val_loss:.2e}\\n')\n",
        "\n",
        "        \n",
        "        if i_epoch % save_interval == 0:\n",
        "            torch.save(autoencoder.encoder.state_dict(), save_path)\n",
        "            decoder_path = save_path.with_stem(save_path.stem + '_decoder')\n",
        "            torch.save(autoencoder.decoder.state_dict(), decoder_path)\n",
        "\n",
        "            np.savetxt(\n",
        "                loss_path,\n",
        "                np.column_stack([train_losses, val_losses]),\n",
        "                delimiter=',',\n",
        "                header='train_loss,val_loss',\n",
        "            )\n",
        "\n",
        "        # Holdout validation\n",
        "        if i_epoch > 0 and val_losses[i_epoch] > val_losses[i_epoch - 1]:\n",
        "            n_epochs_val_rising += 1\n",
        "\n",
        "        if n_epochs_val_rising == n_rising_until_break:\n",
        "            break\n",
        "\n",
        "    print(f'Saved weights to {save_path}')\n",
        "\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "\n",
        "encoder_dims = [64, 64, 32, 16, 8]\n",
        "latent_dim = 4\n",
        "\n",
        "autoencoder = ScoreboardAutoencoder(\n",
        "    n_entries=ABCYatzyPlayer.NUM_ENTRIES,\n",
        "    latent_dim=latent_dim,\n",
        "    mlp_dims=encoder_dims,\n",
        ")\n",
        "\n",
        "dataset_path = os.path.join('..', 'datasets', '512k_scoreboards.csv')\n",
        "dataset_df = pd.read_csv(dataset_path)\n",
        "dataset = ScoreboardDataset(dataset_df)\n",
        "\n",
        "train_set, val_set = random_split(dataset, [.95, 0.05])\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
        "\n",
        "param_specifier = ''.join([\n",
        "    'auto_encoder_',\n",
        "    '[',\n",
        "    *[str(dim) + '_' for dim in encoder_dims],\n",
        "    ']_',\n",
        "    f'{latent_dim}',\n",
        "    ])\n",
        "\n",
        "now = datetime.now()\n",
        "formatted_time = now.strftime(\"%d-%m-%H-%M\")\n",
        "\n",
        "loss_file_path = Path(''.join([\n",
        "    'loss_log_',\n",
        "    param_specifier,\n",
        "    '__',\n",
        "    formatted_time,\n",
        "    '.csv'\n",
        "]))\n",
        "loss_log_dir_path = os.path.join('..', 'loss_logs')\n",
        "loss_path = Path(os.path.join(loss_log_dir_path, loss_file_path))\n",
        "\n",
        "weights_dir_path = os.path.join('..', 'weights', 'autoencoder')\n",
        "weights_path = param_specifier + '.pth'\n",
        "save_path = Path(os.path.join(weights_dir_path, weights_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_autoencoder(\n",
        "    autoencoder,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    loss_path=loss_path,\n",
        "    n_rising_until_break=1,\n",
        "    save_interval=5,\n",
        "    save_path=save_path,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
